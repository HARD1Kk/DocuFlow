# Batch Size ‚Äî Complete Notes

---

## 1. Definition

```
Batch Size = The number of items processed together in ONE operation
             before moving to the next group.
```

---

## 2. Simple Analogy

```
You are a teacher grading 100 exam papers.

batch_size = 1:
‚îú‚îÄ‚îÄ Pick 1 paper ‚Üí grade it ‚Üí put away
‚îú‚îÄ‚îÄ Pick 1 paper ‚Üí grade it ‚Üí put away
‚îú‚îÄ‚îÄ Repeat 100 times
‚îî‚îÄ‚îÄ Slow (too much picking up and putting down)

batch_size = 10:
‚îú‚îÄ‚îÄ Pick 10 papers ‚Üí grade all 10 ‚Üí put away
‚îú‚îÄ‚îÄ Pick 10 papers ‚Üí grade all 10 ‚Üí put away
‚îú‚îÄ‚îÄ Repeat 10 times
‚îî‚îÄ‚îÄ Efficient ‚úÖ

batch_size = 100:
‚îú‚îÄ‚îÄ Pick ALL 100 papers at once
‚îú‚îÄ‚îÄ Your desk overflows ‚Üí papers fall ‚Üí mess
‚îî‚îÄ‚îÄ Crashed (out of space) ‚ùå
```

---

## 3. How It Works in Code

### Without Batching

```python
texts = ["t1", "t2", "t3", ..., "t1000"]  # 1000 texts

# ALL at once ‚Äî risky
embeddings = model.encode(texts)  # loads all 1000 into memory
```

### With Batching

```python
texts = ["t1", "t2", "t3", ..., "t1000"]
batch_size = 64

results = []
for i in range(0, len(texts), batch_size):
    batch = texts[i:i + batch_size]       # take 64 at a time
    embeddings = model.encode(batch)       # process only 64
    results.extend(embeddings.tolist())    # store results
```

---

## 4. Step-by-Step Trace

```
10 texts, batch_size = 3

texts = ["a", "b", "c", "d", "e", "f", "g", "h", "i", "j"]

range(0, 10, 3) ‚Üí [0, 3, 6, 9]

Step 1: i=0 ‚Üí texts[0:3]  = ["a","b","c"] ‚Üí encode ‚Üí 3 embeddings
Step 2: i=3 ‚Üí texts[3:6]  = ["d","e","f"] ‚Üí encode ‚Üí 3 embeddings
Step 3: i=6 ‚Üí texts[6:9]  = ["g","h","i"] ‚Üí encode ‚Üí 3 embeddings
Step 4: i=9 ‚Üí texts[9:12] = ["j"]         ‚Üí encode ‚Üí 1 embedding

Total: 3 + 3 + 3 + 1 = 10 embeddings ‚úÖ
```

---

## 5. How `range(start, stop, step)` Works

```python
range(0, 10, 3)

start = 0   ‚Üí begin at index 0
stop  = 10  ‚Üí stop before index 10
step  = 3   ‚Üí jump by 3

produces ‚Üí [0, 3, 6, 9]
```

---

## 6. Why Batching Is Needed

### Problem 1: Out of Memory

```
Without batching:
1,000,000 texts √ó 4 KB each = 4 GB input
+ model weights             = 1.3 GB
+ intermediate computations = 8 GB
= 13+ GB total

Your RAM = 8 GB ‚Üí üí• CRASH
```

### Problem 2: Overhead Per Call

```
Each model.encode() call has fixed costs:

‚îú‚îÄ‚îÄ Tokenizer setup         ~2ms
‚îú‚îÄ‚îÄ Computation graph setup  ~3ms
‚îú‚îÄ‚îÄ Memory allocation        ~1ms
‚îú‚îÄ‚îÄ Actual computation       varies
‚îî‚îÄ‚îÄ Total overhead:          ~6ms per call

batch_size=1,  1000 texts ‚Üí 1000 calls ‚Üí 6000ms overhead
batch_size=64, 1000 texts ‚Üí 16 calls   ‚Üí 96ms overhead
```

---

## 7. Batch Size Tradeoffs

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    ‚îÇ   Too Small    ‚îÇ    Too Large     ‚îÇ
‚îÇ                    ‚îÇ  (1 or 2)      ‚îÇ  (100,000+)      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Speed              ‚îÇ Slow           ‚îÇ Fast             ‚îÇ
‚îÇ Memory Usage       ‚îÇ Very Low       ‚îÇ Very High        ‚îÇ
‚îÇ Overhead           ‚îÇ High           ‚îÇ Low              ‚îÇ
‚îÇ Crash Risk         ‚îÇ None           ‚îÇ High             ‚îÇ
‚îÇ Verdict            ‚îÇ Safe but slow  ‚îÇ Fast but risky   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

             Sweet Spot = somewhere in the MIDDLE
```

---

## 8. Choosing the Right Batch Size

### Based on Hardware

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Hardware                ‚îÇ Recommended        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ CPU + 4 GB RAM          ‚îÇ batch_size = 16    ‚îÇ
‚îÇ CPU + 8 GB RAM          ‚îÇ batch_size = 32    ‚îÇ
‚îÇ CPU + 16 GB RAM         ‚îÇ batch_size = 64    ‚îÇ
‚îÇ CPU + 32 GB RAM         ‚îÇ batch_size = 128   ‚îÇ
‚îÇ GPU + 4 GB VRAM         ‚îÇ batch_size = 64    ‚îÇ
‚îÇ GPU + 8 GB VRAM         ‚îÇ batch_size = 256   ‚îÇ
‚îÇ GPU + 16 GB VRAM        ‚îÇ batch_size = 512   ‚îÇ
‚îÇ GPU + 24 GB VRAM        ‚îÇ batch_size = 1024  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Based on Text Length

```
Short texts ("hello world"):
‚îî‚îÄ‚îÄ Can use larger batch_size (128-256)

Long texts (full paragraphs/documents):
‚îî‚îÄ‚îÄ Use smaller batch_size (16-32)
    (each text uses more memory)
```

---

## 9. Memory Calculation

```
Memory per batch ‚âà batch_size √ó max_tokens √ó hidden_size √ó 4 bytes

Example with bge-large:
‚îú‚îÄ‚îÄ batch_size   = 64
‚îú‚îÄ‚îÄ max_tokens   = 512
‚îú‚îÄ‚îÄ hidden_size  = 1024
‚îú‚îÄ‚îÄ bytes        = 4 (float32)
‚îÇ
‚îî‚îÄ‚îÄ 64 √ó 512 √ó 1024 √ó 4 = 134 MB per batch

Example with batch_size = 1000:
‚îî‚îÄ‚îÄ 1000 √ó 512 √ó 1024 √ó 4 = 2 GB per batch ‚ö†Ô∏è
```

---

## 10. What Happens Inside Each Batch

```
Batch = ["How are you?", "Good morning", "Hello world"]

Step 1: TOKENIZATION
‚îú‚îÄ‚îÄ "How are you?"  ‚Üí [101, 2129, 2024, 2017, 102, 0, 0]
‚îú‚îÄ‚îÄ "Good morning"  ‚Üí [101, 3342, 2851, 102, 0, 0, 0]
‚îú‚îÄ‚îÄ "Hello world"   ‚Üí [101, 7592, 2088, 102, 0, 0, 0]
‚îÇ                                              ‚Üë
‚îÇ                                         padding (all same length)
‚îî‚îÄ‚îÄ Result: 2D array of shape (3, 7)

Step 2: MODEL PROCESSING
‚îú‚îÄ‚îÄ Input shape:  (3, 7)      ‚Üí 3 texts, 7 tokens each
‚îú‚îÄ‚îÄ Through transformer layers
‚îî‚îÄ‚îÄ Output shape: (3, 1024)   ‚Üí 3 texts, 1024-dim embedding each

Step 3: RESULT
‚îú‚îÄ‚îÄ "How are you?"  ‚Üí [0.23, -0.45, 0.78, ..., 0.12]
‚îú‚îÄ‚îÄ "Good morning"  ‚Üí [0.31, -0.22, 0.65, ..., 0.09]
‚îî‚îÄ‚îÄ "Hello world"   ‚Üí [0.18, -0.56, 0.82, ..., 0.15]
```

---

## 11. Batch Size in Different Contexts

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Context                 ‚îÇ What Batch Size Means              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Text Embedding          ‚îÇ Number of texts encoded together   ‚îÇ
‚îÇ (our case)              ‚îÇ                                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Deep Learning Training  ‚îÇ Number of training samples per     ‚îÇ
‚îÇ                         ‚îÇ gradient update                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Database Operations     ‚îÇ Number of rows inserted/updated    ‚îÇ
‚îÇ                         ‚îÇ per transaction                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ API Calls               ‚îÇ Number of requests sent together   ‚îÇ
‚îÇ                         ‚îÇ in one API call                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ File Processing         ‚îÇ Number of files read/processed     ‚îÇ
‚îÇ                         ‚îÇ before writing results             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 12. Common Errors Related to Batch Size

### Error 1: Out of Memory

```python
# Cause: batch_size too large
torch.cuda.OutOfMemoryError  # GPU
MemoryError                  # CPU

# Fix: reduce batch_size
batch_size = 32  # try smaller
```

### Error 2: Too Slow

```python
# Cause: batch_size too small (usually 1)
# Each call has overhead

# Fix: increase batch_size
batch_size = 64  # try larger
```

### Error 3: Last Batch Smaller

```python
# 10 texts, batch_size = 3
# Last batch has only 1 text ‚Äî is this a problem?

texts[9:12] = ["j"]  # only 1 element, NOT 3

# Answer: NO problem
# Python slicing handles this gracefully
# model.encode(["j"]) works fine with 1 text
```

---

## 13. Complete Template

```python
from typing import List
import numpy as np

def process_in_batches(
    items: List[str],
    batch_size: int = 64
) -> List[List[float]]:
    """
    Process items in batches to avoid memory issues.

    Args:
        items: All items to process.
        batch_size: How many items per batch.

    Returns:
        All results combined.
    """
    if not items:
        return []

    all_results = []

    total_batches = (len(items) + batch_size - 1) // batch_size

    for i in range(0, len(items), batch_size):
        batch_num = i // batch_size + 1
        batch = items[i:i + batch_size]

        print(f"Processing batch {batch_num}/{total_batches} "
              f"({len(batch)} items)")

        results = model.encode(batch)

        if isinstance(results, np.ndarray):
            all_results.extend(results.tolist())
        else:
            all_results.extend(results)

    print(f"Done! Processed {len(items)} items total")
    return all_results
```

---

## 14. Quick Reference

```
BATCH SIZE = items processed at once

Small (1-8):     Safe     + Slow    + High overhead
Medium (32-128): Safe     + Fast    + Low overhead    ‚Üê BEST
Large (512+):    Risky    + Fastest + May crash

Formula:
total_batches = ceil(total_items / batch_size)

CPU recommended:  32-64
GPU recommended:  128-512

Key rule: If you get memory errors ‚Üí REDUCE batch_size
          If processing is too slow ‚Üí INCREASE batch_size
```